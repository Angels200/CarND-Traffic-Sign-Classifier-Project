{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "#import wget \n",
    "from urllib.request import urlretrieve\n",
    "from  urllib.request import urlopen\n",
    "import urllib.error \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from enum import Enum, unique\n",
    "import inspect\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.python.ops.variables import Variable\n",
    "from functools import partial\n",
    "from itertools import groupby\n",
    "import pycuda\n",
    "import pycuda.driver as cuda\n",
    "%matplotlib inline\n",
    "\n",
    "print('All modules imported.')\n",
    "\n",
    "source = 'https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581faac4_traffic-signs-data/traffic-signs-data.zip'\n",
    "repository = 'traffic-signs-data.zip'\n",
    "batch_count = 128\n",
    "rate = 0.001\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self,source, repository):\n",
    "        \n",
    "        print('Status of Initialization Step #0')\n",
    "        if source is None:\n",
    "            print('None source parameter is not allowed. Please, provide a source parameter.')\n",
    "            return None\n",
    "        if len(source)==0:\n",
    "            print(\"Empty source parameter is not allowed. Please, provide a valid source parameter.\")\n",
    "            return None\n",
    "        if repository is None:\n",
    "            print('None repository parameter is not allowed. Please, provide a repository parameter.')\n",
    "            return None\n",
    "        if len(repository)==0:\n",
    "            print(\"Empty repository parameter is not allowed. Please, provide a valid repository parameter.\")\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        self.__source = source\n",
    "        self.__repository=repository\n",
    "        self.__datacached,self.__dataloaded = False,False\n",
    "        self.__downloaded , self.__uncompressed = 0,0\n",
    "        self.train_size, self.test_size,self.class_size,self.label_per_class = 0,0,0,[]\n",
    "        self.image_shape = None\n",
    "        self.max_size_feature = [0,0]\n",
    "        \n",
    "    def __wget(self, source, repository):\n",
    "        try:\n",
    "            print(source)\n",
    "            content = urlopen(source)\n",
    "            repository = open(source.split('/')[-1], 'w')\n",
    "            repository.write(content.read())\n",
    "            content.close()\n",
    "            repository.close()\n",
    "        except Exception as err:\n",
    "            print('ERROR:', str(err))\n",
    "            raise \n",
    "        \n",
    "    \n",
    "    def __download(self):\n",
    "        print('Download Step #1')\n",
    "        #print(self.__source)\n",
    "        for src, rep in tqdm(zip(self.__source, self.__repository)):\n",
    "            try :\n",
    "                if not os.path.isfile(rep):\n",
    "                    print('Start download files from '+src+'...')\n",
    "                    urlretrieve(src,rep)\n",
    "                    #rep = wget.download(src)\n",
    "                    #self.__wget(src,rep)\n",
    "                    print('Download finished.')\n",
    "                    self.__downloaded += 1\n",
    "                else :\n",
    "                    print('Repository path '+rep+ ' is file! We can not save\\\n",
    "                            files because no directory path is provided.')\n",
    "            except urllib.error.URLError: #Exception as inst : #\n",
    "                    print('file not found corresponding to ' + src +'. moving on...')\n",
    "                    raise\n",
    "                    \n",
    "        if self.__downloaded != 0 :\n",
    "            print('Status of download Step : ' +str(self.__downloaded) + ' files are downloaded over '+\\\n",
    "                      str(len(self.__source)))\n",
    "            \n",
    "    def __uncompress(self):\n",
    "        print('Uncompression Step #3')\n",
    "        destfiles=[]\n",
    "        for path in self.__repository :\n",
    "        #Instantiate zipfile class as zipf context\n",
    "            #print(path)\n",
    "            with ZipFile(path,'r') as zipf:\n",
    "                #extracting file name list using progressbar\n",
    "                pbfiles = zipf.namelist()\n",
    "                for fname in tqdm(pbfiles, unit='files'):\n",
    "                    #check whether  file name do not point to a directory. Thus do not process it \n",
    "                    if not fname.endswith('/'):\n",
    "                        #unzip the image file using the file name\n",
    "                        print('Unzip the image file '+fname)\n",
    "                        #with zipf.open(fname) as file:\n",
    "                        zipf.extract(fname)\n",
    "\n",
    "                        # Now store the uncompressed data\n",
    "                        #fdest = fname[:-3]  # remove the '.gz' from the filename\n",
    "                        #print(fdest)\n",
    "                        destfiles.append(fname)\n",
    "                        \n",
    "            print('Status of uncompression Step :  '+ str(self.__uncompressed)+ \\\n",
    "                      'files are uncompressed over' +str(len(pbfiles)))\n",
    "        destfiles = sorted(destfiles,key=os.path.getsize, reverse=True)\n",
    "        return np.array(destfiles) \n",
    "    \n",
    "    def __load(self,pkfile):\n",
    "        # Reload the data\n",
    "        print('Data Loading Step #4')\n",
    "        train_features,train_labels = None,None\n",
    "        if os.path.isfile(pkfile):\n",
    "            #print(pkfile)\n",
    "            with open(pkfile, 'rb') as f:\n",
    "              pickle_data = pickle.load(f)\n",
    "              features = pickle_data['features']\n",
    "              labels = pickle_data['labels']\n",
    "              del pickle_data  # Free up memory\n",
    "            print('Status of Data loading from pickle file Step : successful')\n",
    "            self.__dataloaded = True\n",
    "        else :\n",
    "            print('Status of Data loading from pickle file Step : failed')\n",
    "        return features,labels\n",
    "    \n",
    "    def deserialize(self):\n",
    "        desfiles = []\n",
    "        print('The dataset deserialization process is starting...')\n",
    "        \n",
    "        if self.__downloaded==0:\n",
    "            self.__download()\n",
    "            \n",
    "        if self.__uncompressed == 0:\n",
    "            desfiles = self.__uncompress()\n",
    "            \n",
    "        if len(desfiles)==2 :\n",
    "            train_features, train_labels = self.__load(desfiles[0])\n",
    "            test_features,test_labels = self.__load(desfiles[1])\n",
    "            self.train_size = train_features.shape[0]\n",
    "            self.test_size = test_features.shape[0]\n",
    "            self.image_shape = (train_features.shape[1],train_features.shape[2],train_features.shape[3])\n",
    "            self.class_size = np.unique(train_labels).size\n",
    "            self.label_per_class = np.array([(k,len(list(v))) for k,v in groupby(train_labels)])\n",
    "           \n",
    "            for t in self.label_per_class:\n",
    "                \n",
    "                if(self.max_size_feature[1]<t[1]):\n",
    "                    self.max_size_feature = t\n",
    "            \n",
    "            \n",
    "        print('The dataset deserialization process is terminated...')\n",
    "        \n",
    "        return train_features,train_labels, test_features,test_labels\n",
    "    \n",
    "    def visualize(self, X_train, y_train) :\n",
    "        index = random.randint(0, len(X_train))\n",
    "        image = X_train[index].squeeze()\n",
    "\n",
    "        plt.figure(figsize=(1,1))\n",
    "        plt.imshow(image)\n",
    "        print(y_train[index])\n",
    "        \n",
    "    def __max(self, X_train, y_train, augmenter):\n",
    "        #Define the max(class,label/feature)\n",
    "        \n",
    "        for t in self.label_per_class:\n",
    "            if(self.max_size_feature[1]<t[1]):\n",
    "                self.max_size_feature = t\n",
    "    \n",
    "    def balance(self,X_train,y_train,augmenter,args):\n",
    "        #for each (class,feature/label) different from max, \n",
    "        # augment(class, feature/label) by the amount of tuple = max(class,feature) - nb(class,feature)\n",
    "        # Adjust(class, label)\n",
    "        # Append(Augmented(class,feature), train_feature)\n",
    "        #params = (1,1)\n",
    "        #print((augmenter) (*params))\n",
    "        augfeatures,auglabels =[],[]\n",
    "        \n",
    "        start=0\n",
    "        for t in self.label_per_class:\n",
    "            augfeatures.append(X_train[t[1]])\n",
    "            auglabels.append(y_train[t[1]])\n",
    "            if t[1]!=self.max_size_feature[1]:\n",
    "                compensation = self.max_size_feature[1]-t[1]\n",
    "                print(compensation)\n",
    "                for i in range(compensation-1):\n",
    "                    bf = X_train[start:start+t[1]]\n",
    "                    bl = y_train[start:start+t[1]]\n",
    "                    args = (bf,args[1])\n",
    "                    ft = (augmenter)(*args)\n",
    "                    augfeatures.append(ft)\n",
    "                    auglabels.append(bl)\n",
    "                    start += t[1]\n",
    "                    \n",
    "        return augfeatures,auglabels\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsources = [source]\\nrepositories = [repository]\\n\\nds = Dataset(sources, repositories)\\nX_train, y_train,X_test, y_test = ds.deserialize()\\nprint(\"Number of training examples =\", ds.train_size)\\nprint(\"Number of testing examples =\", ds.test_size)\\nprint(\"Image data shape =\", ds.image_shape)\\nprint(\"Number of classes =\", ds.class_size)\\n#print(\"Number of label_per_class =\", ds.label_per_class)\\nprint(\"Max size of label_per_class =\", ds.max_size_feature)\\n\\nds.visualize(X_train, y_train)\\n'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sources = [source]\n",
    "repositories = [repository]\n",
    "\n",
    "ds = Dataset(sources, repositories)\n",
    "X_train, y_train,X_test, y_test = ds.deserialize()\n",
    "print(\"Number of training examples =\", ds.train_size)\n",
    "print(\"Number of testing examples =\", ds.test_size)\n",
    "print(\"Image data shape =\", ds.image_shape)\n",
    "print(\"Number of classes =\", ds.class_size)\n",
    "#print(\"Number of label_per_class =\", ds.label_per_class)\n",
    "print(\"Max size of label_per_class =\", ds.max_size_feature)\n",
    "\n",
    "ds.visualize(X_train, y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1: Data Augmentation\n",
    "    This technique provides a way genrerate more data from the existing and thus makes cnn or dnn to be \n",
    "    trained on huge number of training dataset to reach better performance and accuracy.\n",
    "'''\n",
    "\n",
    "class DataAugmenter(object):\n",
    "    '''\n",
    "    This library class is a data augmenter pipeline manager using the Augmenters Module referenced at this url:\n",
    "    https://github.com/aleju/imgaug\n",
    "   '''\n",
    "    class Tasks(object):\n",
    "        def __str__(self):\n",
    "            return str(self.value)\n",
    "        \n",
    "        FlipVertical = 'flip_vertical'\n",
    "        FlipHorizontal = 'flip_horizontal'\n",
    "        Crop = 'crop'\n",
    "        GaussianBlur = 'gaussian_blur'\n",
    "        AdditiveGaussianNoise = 'additive_gaussian_noise'\n",
    "        Dropout = 'dropout'\n",
    "        Add = 'add'\n",
    "        Multiply = 'multiply'\n",
    "        ContrastNormalization = 'contrast_normalization'\n",
    "        Affine = 'affine'\n",
    "        ElasticTransformation = 'elastic_transformation'\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        \n",
    "        # random example images\n",
    "        #images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\n",
    "\n",
    "        # Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n",
    "        # e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image.\n",
    "        self.st = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "        self.pipeline = []\n",
    "        # Define our sequence of augmentation steps that will be applied to every image\n",
    "        # All augmenters with per_channel=0.5 will sample one value _per image_\n",
    "        # in 50% of all cases. In all other cases they will sample new values\n",
    "        # _per channel_.\n",
    "        self.tasks = []\n",
    "    \n",
    "    def flip_horizontal(self):\n",
    "        # horizontally flip 50% of all images\n",
    "        if not \"iaa.Fliplr\" in self.pipeline:\n",
    "            self.pipeline.append(iaa.Fliplr(0.5))\n",
    "            \n",
    "    def flip_vertical(self):\n",
    "        # vertically flip 50% of all images\n",
    "        if not \"iaa.Flipud\"  in self.pipeline:\n",
    "            self.pipeline.append(iaa.Flipud(0.5))\n",
    "            \n",
    "    def crop(self):\n",
    "        # crop images by crop range of 0-10%  of their height/width\n",
    "        if not \"iaa.Crop\" in self.pipeline:\n",
    "            self.pipeline.append(self.st(iaa.Crop(percent=(0, 0.1))))\n",
    "    \n",
    "    def gaussian_blur(self):\n",
    "        # blur images with a sigma range between 0 and 3.0\n",
    "        if not \"iaa.GaussianBlur\" in self.pipeline:\n",
    "            self.pipeline.append(self.st(iaa.GaussianBlur((0, 3.0))))\n",
    "    \n",
    "    def additive_gaussian_noise(self):\n",
    "        # add gaussian noise to images\n",
    "        if \"iaa.AdditiveGaussianNoise\" in self.pipeline:\n",
    "            self.pipeline.append(self.st(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.2), per_channel=0.5)))\n",
    "    \n",
    "    def dropout(self):\n",
    "        # randomly remove up to 10% of the pixels  \n",
    "        if not \"iaa.Dropout\" in self.pipeline:\n",
    "            self.pipeline.append(self.st(iaa.Dropout((0.0, 0.1), per_channel=0.5)))\n",
    "    \n",
    "    def add(self):\n",
    "        # change brightness of images (by -10 to 10 of original value)\n",
    "        if not \"iaa.Add\" in self.pipeline:\n",
    "            self.pipeline.append(self.st(iaa.Add((-10, 10), per_channel=0.5)))\n",
    "    \n",
    "    def multiply(self):\n",
    "        # change brightness of images (50-150% of original value)\n",
    "        if not \"iaa.Multiply\" in self.pipeline:\n",
    "            self.pipeline.append(self.st(iaa.Multiply((0.5, 1.5), per_channel=0.5)))\n",
    "        \n",
    "    def contrast_normalization(self):\n",
    "        # improve or worsen the contrast\n",
    "        if not \"iaa.ContrastNormalization\" in self.pipeline:\n",
    "            self.pipeline.append(self.st(iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5)))\n",
    "    \n",
    "    def affine(self):\n",
    "        \n",
    "        if not \"iaa.Affine\" in self.pipeline:\n",
    "            self.pipeline.append(self.st(iaa.Affine(\n",
    "                    scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
    "                    translate_px={\"x\": (-16, 16), \"y\": (-16, 16)}, # translate by -16 to +16 pixels (per axis)\n",
    "                    rotate=(-45, 45), # rotate by -45 to +45 degrees\n",
    "                    shear=(-16, 16), # shear by -16 to +16 degrees\n",
    "                    order=ia.ALL, # use any of scikit-image's interpolation methods\n",
    "                    cval=(0, 1.0), # if mode is constant, use a cval between 0 and 1.0\n",
    "                    mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "                )))\n",
    "            \n",
    "    def elastic_transformation(self):\n",
    "        # apply elastic transformations with random strengths\n",
    "        if not \"iaa.ElasticTransformation\" in self.pipeline:\n",
    "            self.pipeline.append(self.st(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)))\n",
    "    \n",
    "        \n",
    "    def register(self,tasks):\n",
    "        #Allow to register the tailored data augmentation pipeline        \n",
    "        print('Augmenter Pipeline registration process is starting ...')\n",
    "        if tasks is None:\n",
    "            print('None tasks parameter is not allowed. Please, provide a tasks parameter.')\n",
    "            return None\n",
    "        if len(tasks)==0:\n",
    "            print(\"Empty tasks parameter is not allowed. Please, provide a valid tasks parameter.\")\n",
    "            return None\n",
    "        \n",
    "        self.tasks = tasks\n",
    "        \n",
    "        methnames = [m for m in dir(self) if inspect.ismethod(getattr(self, m))]                     \n",
    "\n",
    "        for taskname in tasks :                         \n",
    "            if taskname not in methnames:\n",
    "                print('The following method does not exists: ', taskname)\n",
    "                return\n",
    "        for taskname in tasks :\n",
    "            self.__getattribute__(taskname)()\n",
    "        \n",
    "        print('Augmenter Pipeline registration process is terminated ...')\n",
    "    \n",
    "    def execute(self, images, show=False):\n",
    "        print('Augmenter Pipeline execution process is starting ...')\n",
    "        pline = iaa.Sequential(self.pipeline, \n",
    "                             random_order = True # do all of the above in random order\n",
    "                                )                         \n",
    "        augmented_images = pline.augment_images(images)\n",
    "        \n",
    "        if show :\n",
    "            # show an image with 8*8 augmented versions of image 0\n",
    "            seq.show_grid(images[0], cols=8, rows=8)\n",
    "\n",
    "        print('Augmenter Pipeline execution process is terminated ...')\n",
    "        \n",
    "        return augmented_images\n",
    "    \n",
    "    def augmente(self,X_train) :\n",
    "    \n",
    "        da = self #DataAugmenter()\n",
    "\n",
    "        tasks = [da.Tasks.FlipHorizontal,da.Tasks.FlipVertical,da.Tasks.Crop,\n",
    "                    da.Tasks.GaussianBlur,da.Tasks.Dropout,\n",
    "                 da.Tasks.Affine,da.Tasks.ElasticTransformation]\n",
    "\n",
    "        da.register(tasks)\n",
    "        \n",
    "        return da.execute(X_train)\n",
    "        \n",
    "        '''\n",
    "        batch_features, batch_labels  = self.load_batch(0,X_train, y_train)\n",
    "\n",
    "        if apply :\n",
    "            images_aug = da.execute(batch_features)\n",
    "        else:\n",
    "            images_aug = batch_features\n",
    "            print('Not Apply')\n",
    "        #train_on_images(images_aug)\n",
    "\n",
    "        for idx,image in enumerate(images_aug):\n",
    "            plt.figure(figsize=(1,1))\n",
    "            plt.imshow(image)\n",
    "            print(batch_labels[idx])\n",
    "        '''\n",
    "        \n",
    "    def load_batch(self, batch_i,train_features,train_labels):\n",
    "        batch_size = 1000 #train_features/batch_count\n",
    "        # The training cycle\n",
    "        #for batch_i in range(batch_count):\n",
    "        # Get a batch of training features and labels\n",
    "        batch_start = batch_i*batch_size\n",
    "        batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "        batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "        return batch_features, batch_labels   \n",
    "\n",
    "\n",
    "def apply_augmentation2(batch_features, apply=False) :\n",
    "    \n",
    "    da = DataAugmenter()\n",
    "    \n",
    "    tasks = [da.Tasks.FlipHorizontal,da.Tasks.FlipVertical,da.Tasks.Crop,\n",
    "                da.Tasks.GaussianBlur,da.Tasks.Dropout,\n",
    "             da.Tasks.Affine,da.Tasks.ElasticTransformation]\n",
    "    \n",
    "    da.register(tasks)\n",
    "    \n",
    "    #batch_features, batch_labels  = load_batch(0,X_train, y_train)\n",
    "    \n",
    "    if apply :\n",
    "        images_aug = da.execute(batch_features)\n",
    "    else:\n",
    "        images_aug = batch_features\n",
    "        print('Not Apply')\n",
    "    #train_on_images(images_aug)\n",
    "    '''\n",
    "    for idx,image in enumerate(images_aug):\n",
    "        plt.figure(figsize=(1,1))\n",
    "        plt.imshow(image)\n",
    "        #print(batch_labels[idx])\n",
    "    '''\n",
    "    \n",
    "    print(len(images_aug))\n",
    "    \n",
    "    return images_aug\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#apply_augmentation(X_train, y_train,True) \n",
    "#args = (None,True)\n",
    "#ds.balance(X_train,y_train,partial(apply_augmentation2),args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPreProcessor(object):\n",
    "    '''\n",
    "    http://cs231n.github.io/neural-networks-2/#datapre\n",
    "    There are three common forms of data preprocessing a data matrix X, \n",
    "    where we will assume that X is of size [N x D] (N is the number of data, D is their dimensionality).\n",
    "    '''\n",
    "    \n",
    "    class Tasks(object):\n",
    "        SimpleRescale = 'simple_rescale'\n",
    "        MeanSubstract = 'mean_substract'\n",
    "        FeatureStandardization = 'feature_standardization'\n",
    "        \n",
    "    class Normalizer(object):\n",
    "        \n",
    "        def _simple_rescale(self,images):\n",
    "            images /= 255\n",
    "            return images\n",
    "        \n",
    "        def _mean_substract(self,images):\n",
    "            print('_zero_centred')\n",
    "            print(images.dtype)\n",
    "            #np.mean(images, axis = 0,dtype=np.uint8)\n",
    "            images -= np.mean(images, axis = 0,dtype=np.uint8)\n",
    "            return images\n",
    "        \n",
    "        def _feature_standardization(self, images):\n",
    "            print('_normalize')\n",
    "            images /= np.std(images, axis = 0)\n",
    "            return images\n",
    "        \n",
    "    class Sequential(object):\n",
    "        \n",
    "        def __init__(self, sequence):\n",
    "            self.sequence = sequence\n",
    "        \n",
    "        def preprocess_images(self,images):\n",
    "            obj = self.sequence[0]\n",
    "            self.sequence.remove(obj)\n",
    "            \n",
    "            for seq in self.sequence :\n",
    "                images = getattr(obj, seq)(images)\n",
    "                \n",
    "            return images\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline = []\n",
    "        ppc = self.Normalizer()\n",
    "        self.pipeline.append(ppc)\n",
    "    \n",
    "    def simple_rescale(self):\n",
    "        if not \"_simple_rescale\" in  self.pipeline:\n",
    "            self.pipeline.append(\"_simple_rescale\") \n",
    "    \n",
    "    def mean_substract(self):\n",
    "        if not \"_mean_substract\" in  self.pipeline:\n",
    "            self.pipeline.append(\"_mean_substract\") \n",
    "        \n",
    "    def feature_standardization(self):\n",
    "        if \"_feature_standardization\" in  self.pipeline :\n",
    "            self.pipeline.append(\"_feature_standardization\") \n",
    "        \n",
    "    def register(self,tasks):\n",
    "        print('Preprocessor Pipeline registration process is starting ...')\n",
    "        if tasks is None:\n",
    "            print('None tasks parameter is not allowed. Please, provide a tasks parameter.')\n",
    "            return None\n",
    "        if len(tasks)==0:\n",
    "            print(\"Empty tasks parameter is not allowed. Please, provide a valid tasks parameter.\")\n",
    "            return None\n",
    "        \n",
    "        self.tasks = tasks\n",
    "        \n",
    "        methnames = [m for m in dir(self) if inspect.ismethod(getattr(self, m))]                     \n",
    "        print(methnames)\n",
    "        print(tasks)\n",
    "        for taskname in tasks :                         \n",
    "            if taskname not in methnames:\n",
    "                print('The following method does not exists: ', taskname)\n",
    "                return\n",
    "        for taskname in tasks :\n",
    "            self.__getattribute__(taskname)()\n",
    "        \n",
    "        print('Preprocessor Pipeline registration process is terminated ...')\n",
    "        \n",
    "    def execute(self, images, show=False):\n",
    "        print('Preprocessor Pipeline execution process is starting ...')\n",
    "        pline = self.Sequential(self.pipeline)\n",
    "        \n",
    "        imgs = pline.preprocess_images(images)\n",
    "        \n",
    "        if show :\n",
    "            # show an image with 8*8 augmented versions of image 0\n",
    "            seq.show_grid(imgs[0], cols=8, rows=8)\n",
    "\n",
    "        print('Preprocessor Pipeline execution process is terminated ...')\n",
    "        \n",
    "        return imgs\n",
    "    \n",
    "    def preprocess(self, X_train, y_train, apply=False) :\n",
    "        dpp = self #DataPreProcessor()\n",
    "        pipeline = [dpp.Tasks.MeanSubstract,dpp.Tasks.FeatureStandardization]\n",
    "        dpp.register(pipeline)\n",
    "        batch_features = X_train\n",
    "        #batch_features, batch_labels  = load_batch(0,X_train, y_train)\n",
    "\n",
    "        if apply :\n",
    "            return dpp.execute(batch_features) # images_aug = dpp.execute(batch_features)\n",
    "        else:\n",
    "            return batch_features #images_aug = batch_features\n",
    "            print('Not Apply')\n",
    "        #train_on_images(images_aug)\n",
    "        '''\n",
    "        for idx,image in enumerate(images_aug):\n",
    "            plt.figure(figsize=(1,1))\n",
    "            plt.imshow(image)\n",
    "            print(batch_labels[idx])\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#args = (None,True)\n",
    "#ds.balance(X_train,y_train,partial(apply_augmentation2),args)\n",
    "#apply_preprocessing(X_train, y_train,True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "class Layer(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self):\n",
    "        self._input=None\n",
    "        self._output=None\n",
    "        \n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "        \n",
    "    @input.setter\n",
    "    def input(self,val):\n",
    "        _input_setter(val)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _input_setter(self,val):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def output(self):\n",
    "        return self._output\n",
    "    '''\n",
    "    @output.setter\n",
    "    def output(self,val):\n",
    "        _output_setter(val)\n",
    "        \n",
    "    @abstractmethod\n",
    "    def _output(self,val):\n",
    "        pass\n",
    "    '''\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute(self,images):pass\n",
    "        \n",
    "class Convolutional(Layer):\n",
    "    #def __init__(self,stride,ksize,padding,sigma,mu):\n",
    "    def __init__(self,params):\n",
    "        \n",
    "        print(params)\n",
    "        print('stride : ',params['stride'])\n",
    "        assert(params['stride']!=None or not len(params['stride'])==0),\"None or empty stride parameter is not allowed. Please, provide a stride parameter\"\n",
    "        assert(params['ksize'] !=None or not len(params['ksize'])==0),\"None or empty ksize parameter is not allowed. Please, provide a ksize parameter\"\n",
    "        assert(params['padding'] !=None or not len(params['padding'])==0),\"None or empty padding parameter is not allowed. Please, provide a padding parameter\"\n",
    "        assert(params['sigma'] !=None),\"None  sigma parameter is not allowed. Please, provide a sigma parameter\"\n",
    "        assert(params['mu']!=None ),\"None mu parameter is not allowed. Please, provide a mu parameter\"\n",
    "        \n",
    "        self.stride = params['stride']\n",
    "        self.ksize = params['ksize']\n",
    "        self.weights = None\n",
    "        self.biais = None\n",
    "        self.sigma = params['sigma']\n",
    "        self.mu = params['mu']\n",
    "        self.padding = params['padding']\n",
    "        \n",
    "    def _input_setter(self,val):\n",
    "        assert(val !=None or not len(val)==0),\"None or empty val parameter is not allowed. Please, provide a val parameter\"\n",
    "        self._input = val\n",
    "        \n",
    "    def compute(self, images):\n",
    "        assert(images !=None or not len(images)==0),\"None or empty images parameter is not allowed. Please, provide a images parameter\"\n",
    "        self.weights = tf.Variable(tf.truncated_normal(shape = self.ksize, mean = self.mu, stddev = self.sigma))\n",
    "        self.biais = tf.Variable(tf.zeros(self.ksize[3]))\n",
    "        print('images :',images)\n",
    "        print('weights :',self.weights)\n",
    "        print('biais :',self.biais)\n",
    "        return tf.nn.conv2d(images, self.weights, strides=self.stride, padding=self.padding) + self.biais\n",
    "\n",
    "class  Activation(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _input_setter(self,val):\n",
    "        assert(val !=None or not len(val)==0),\"None or empty val parameter is not allowed. Please, provide a val parameter\"\n",
    "        self._input = val\n",
    "    \n",
    "    def compute(self,images):\n",
    "        assert(images !=None or not len(images)==0),\"None or empty images parameter is not allowed. Please, provide a images parameter\" \n",
    "        return tf.nn.relu(images)\n",
    "        \n",
    "class Pooling(Layer):\n",
    "    def __init__(self,params):\n",
    "        assert(params['stride'] !=None or not len(params['stride'])==0),\"None or empty stride parameter is not allowed. Please, provide a source parameter\"\n",
    "        assert(params['ksize'] !=None or not len(params['ksize'])==0),\"None or empty ksize parameter is not allowed. Please, provide a ksize parameter\"\n",
    "        assert(params['padding'] !=None or not len(params['padding'])==0),\"None or empty padding parameter is not allowed. Please, provide a padding parameter\"\n",
    "                \n",
    "        self.stride = params['stride']\n",
    "        self.ksize = params['ksize']\n",
    "        self.padding = params['padding'] \n",
    "    \n",
    "    def _input_setter(self,val):\n",
    "        assert(val==None or len(val)==0),\"None or empty input parameter is not allowed. Please, provide a input parameter\"\n",
    "        self._input = val\n",
    "        \n",
    "    def compute(self,images):\n",
    "        assert(images !=None or not len(images)==0),\"None or empty images parameter is not allowed. Please, provide a images parameter\"\n",
    "        return tf.nn.max_pool(images, ksize=self.ksize, strides=self.stride, padding=self.padding)\n",
    "        \n",
    "class Flattening(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def compute(self,images):\n",
    "        assert(images !=None or not len(images)==0),\"None or empty images parameter is not allowed. Please, provide a images parameter\" \n",
    "        return flatten(images)\n",
    "        \n",
    "class FullyConnected(Layer):\n",
    "    def __init__(self,params):\n",
    "        assert(params['ksize'] !=None or not len(params['ksize'])==0),\"None or empty ksize parameter is not allowed. Please, provide a ksize parameter\"\n",
    "        assert(params['sigma'] !=None),\"None  sigma parameter is not allowed. Please, provide a sigma parameter\"\n",
    "        assert(params['mu'] !=None ),\"None mu parameter is not allowed. Please, provide a mu parameter\"\n",
    "        \n",
    "        self.ksize = params['ksize']\n",
    "        self.weights = None\n",
    "        self.biais = None\n",
    "        self.sigma = params['sigma']\n",
    "        self.mu = params['mu']\n",
    "       \n",
    "    \n",
    "    def _input_setter(self,val):\n",
    "        assert(val !=None or not len(val)==0),\"None or empty val parameter is not allowed. Please, provide a val parameter\"\n",
    "        self._input = val\n",
    "        \n",
    "    def compute(self,images):\n",
    "        assert(images !=None or not len(images)==0),\"None or empty images parameter is not allowed. Please, provide a images parameter\" \n",
    "        self.weights = tf.Variable(tf.truncated_normal(shape=self.ksize, mean = self.mu, stddev = self.sigma))\n",
    "        self.biais = tf.Variable(tf.zeros(self.ksize[1]))\n",
    "        return tf.matmul(images, self.weights) + self.biais\n",
    "        \n",
    "        \n",
    "class Model(object):\n",
    "    \n",
    "    class Tasks(object):\n",
    "        Convolve = 'convolve'\n",
    "        Activate = 'activate'\n",
    "        Pool = 'pool'\n",
    "        Flatten = 'flatten'\n",
    "        FullConnect = 'fullconnect'\n",
    "    \n",
    "    class Sequential(object):\n",
    "        def __init__(self, sequence):\n",
    "            self.sequence = sequence\n",
    "        \n",
    "        def run_pipeline(self,images):\n",
    "            obj = self.sequence.get_value('obj__1')\n",
    "            self.sequence.remove('obj__1')\n",
    "            \n",
    "            for taskname in self.sequence.get_keys() :\n",
    "                print(taskname)\n",
    "                tsk = taskname.split('__')[0]\n",
    "                print('tsk :',tsk)\n",
    "                images = getattr(obj, tsk)(self.sequence.get_value(taskname),images)\n",
    "                print('tensor :',images)\n",
    "                \n",
    "            return images\n",
    "        \n",
    "    class Container(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self._dic = collections.OrderedDict()\n",
    "            \n",
    "            self.identifier = 0\n",
    "\n",
    "        def __get_id(self):\n",
    "            self.identifier += 1\n",
    "            return '__' + str(self.identifier)\n",
    "        \n",
    "        def insert(self, key,value):\n",
    "            assert(not value == None ),\"None or empty value parameter is not allowed. Please, provide a value parameter\" \n",
    "            __key = key+self.__get_id()\n",
    "            self._dic[__key] = value\n",
    "                        \n",
    "        def update(self, key,value):\n",
    "            assert(not key == None or key>0),\"None or empty key parameter is not allowed. Please, provide a key parameter\" \n",
    "            assert(not value == None),\"None or empty value parameter is not allowed. Please, provide a value parameter\" \n",
    "            self._dic[key] = value\n",
    "        \n",
    "        def remove(self,key):\n",
    "            assert(not key == None or key>0),\"None or empty key parameter is not allowed. Please, provide a key parameter\" \n",
    "            del self._dic[key]\n",
    "            \n",
    "        def remove_all(self, key):\n",
    "            assert(not key == None or key>0),\"None or empty key parameter is not allowed. Please, provide a key parameter\" \n",
    "            self._dic.clear()\n",
    "        \n",
    "        def get_keys(self):\n",
    "            return self._dic.keys()\n",
    "        \n",
    "        def get_value(self,key):\n",
    "            return self._dic[key]\n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.pipeline = self.Container() #[]\n",
    "        #self.tasks = Container() #[]\n",
    "        self.pipeline.insert('obj',self)\n",
    "        self.logits = None\n",
    "        \n",
    "    #stride,ksize,padding,sigma,mu\n",
    "    #stride,ksize,padding\n",
    "    def _convolve(self,params,images):\n",
    "        conv = Convolutional(params)\n",
    "        return conv.compute(images)\n",
    "    \n",
    "    def _activate(self, params,images):\n",
    "        act = Activation()\n",
    "        return act.compute(images)\n",
    "    \n",
    "    def _pool(self, params, images):\n",
    "        pl=Pooling(params)\n",
    "        return pl.compute(images)\n",
    "    \n",
    "    def _flatten(self, params, images):\n",
    "        fl = Flattening()\n",
    "        return fl.compute(images)\n",
    "    \n",
    "    def _fullconnect(self, params, images):\n",
    "        fc = FullyConnected(params)\n",
    "        return fc.compute(images)\n",
    "    \n",
    "    def convolve(self,params):\n",
    "        #if not \"_convolve\" in  self.pipeline.keys():\n",
    "        #self.pipeline.append(\"_convolve\") \n",
    "        self.pipeline.insert('_convolve',params)\n",
    "   \n",
    "    def activate(self,params):\n",
    "        #if not \"_activate__\"+index in  self.pipeline.keys():\n",
    "        self.pipeline.insert('_activate',params)\n",
    "    \n",
    "    def pool(self,params):\n",
    "        #if not \"_pool__\"+index in  self.pipeline.keys():\n",
    "        self.pipeline.insert('_pool',params)\n",
    "    \n",
    "    def flatten(self,params):\n",
    "        #if not \"_flatten__\"+index in  self.pipeline.keys():\n",
    "        self.pipeline.insert('_flatten',params)\n",
    "    \n",
    "    def fullconnect(self,params):\n",
    "        #if not \"_fullconnect__\"+index in  self.pipeline.keys():\n",
    "        self.pipeline.insert('_fullconnect',params)\n",
    "    \n",
    "    \n",
    "    def register(self,tasks):\n",
    "        #Allow to register the tailored data augmentation pipeline        \n",
    "        print('Model Pipeline registration process is starting ...')\n",
    "        if tasks is None:\n",
    "            print('None tasks parameter is not allowed. Please, provide a tasks parameter.')\n",
    "            return None\n",
    "        if len(tasks.get_keys())==0:\n",
    "            print(\"Empty tasks parameter is not allowed. Please, provide a valid tasks parameter.\")\n",
    "            return None\n",
    "        \n",
    "        self.tasks = tasks\n",
    "        \n",
    "        methnames = [m for m in dir(self) if inspect.ismethod(getattr(self, m))]                     \n",
    "\n",
    "        for taskname in tasks.get_keys():  \n",
    "            taskname=taskname.split('_')[0]\n",
    "            if taskname not in methnames:\n",
    "                print('The following method does not exists: ', taskname)\n",
    "                return\n",
    "        for taskname in tasks.get_keys() :\n",
    "            # taskname.split('_')[0] : methodname\n",
    "            #taskname : key of dictionary\n",
    "            self.__getattribute__(taskname.split('_')[0])(tasks.get_value(taskname))\n",
    "        \n",
    "        print('Model Pipeline registration process is terminated ...')\n",
    "        \n",
    "    def execute(self, images, show=False):\n",
    "        print('Model Pipeline execution process is starting ...')\n",
    "        pline = self.Sequential(self.pipeline)\n",
    "        \n",
    "        self.logits = pline.run_pipeline(images)\n",
    "        \n",
    "        if show :\n",
    "            # show an image with 8*8 augmented versions of image 0\n",
    "            seq.show_grid(imgs[0], cols=8, rows=8)\n",
    "\n",
    "        print('Model Pipeline execution process is terminated ...')\n",
    "    \n",
    "    \n",
    "    \n",
    "    def build(self,images,classize) :\n",
    "\n",
    "        #images = tf.placeholder(tf.float32, shape)\n",
    "\n",
    "        #model = Model()\n",
    "        pipeline = self.Container()\n",
    "\n",
    "        print('# SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.')\n",
    "        # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "        params = dict()\n",
    "        params['stride'] = [1, 1, 1, 1]\n",
    "        params['ksize'] = (5, 5, 3, 6)\n",
    "        params['padding'] = 'VALID'\n",
    "        params['sigma'] = 0.1\n",
    "        params['mu'] = 0\n",
    "        pipeline.insert(self.Tasks.Convolve,params)  #stride,ksize,padding,sigma,mu\n",
    "\n",
    "        print('# SOLUTION: Activation.')\n",
    "        # SOLUTION: Activation.\n",
    "        pipeline.insert(self.Tasks.Activate,0) \n",
    "\n",
    "\n",
    "        print('# SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.')\n",
    "        # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "        params = dict()\n",
    "        params['stride'] = [1, 2, 2, 1]\n",
    "        params['ksize'] = [1, 2, 2, 1]\n",
    "        params['padding'] = 'VALID'\n",
    "        pipeline.insert(self.Tasks.Pool,params) #stride,ksize,padding\n",
    "\n",
    "        print('# SOLUTION: Layer 2: Convolutional. Output = 10x10x16.')\n",
    "        # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "        params = dict()\n",
    "        params['stride'] = [1, 1, 1, 1]\n",
    "        params['ksize'] = (5, 5, 6, 16)\n",
    "        params['padding'] = 'VALID'\n",
    "        params['sigma'] = 0.1\n",
    "        params['mu'] = 0\n",
    "        pipeline.insert(self.Tasks.Convolve, params) #stride,ksize,padding,sigma,mu\n",
    "\n",
    "        print('# SOLUTION: Activation.')\n",
    "        # SOLUTION: Activation.\n",
    "        pipeline.insert(self.Tasks.Activate,0)\n",
    "\n",
    "\n",
    "        print('# SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.')\n",
    "        # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "        params = dict()\n",
    "        params['stride'] = [1, 2, 2, 1]\n",
    "        params['ksize'] = [1, 2, 2, 1]\n",
    "        params['padding'] = 'VALID'\n",
    "        pipeline.insert(self.Tasks.Pool, params) #stride,ksize,padding\n",
    "\n",
    "\n",
    "        print('# SOLUTION: Flatten. Input = 5x5x16. Output = 400.')\n",
    "        # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "        pipeline.insert(self.Tasks.Flatten, 0)\n",
    "\n",
    "        print('# SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.')\n",
    "        # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "        params = dict()\n",
    "        params['ksize'] = (400, 120)\n",
    "        params['sigma'] = 0.1\n",
    "        params['mu'] = 0\n",
    "        pipeline.insert(self.Tasks.FullConnect,params) #ksize,sigma,mu\n",
    "\n",
    "        print('# SOLUTION: Activation.')\n",
    "        # SOLUTION: Activation.\n",
    "        pipeline.insert(self.Tasks.Activate, 0)\n",
    "\n",
    "\n",
    "        print('# SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.')\n",
    "        # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        params = dict()\n",
    "        params['ksize'] = (120, 84)\n",
    "        params['sigma'] = 0.1\n",
    "        params['mu'] = 0\n",
    "        pipeline.insert(self.Tasks.FullConnect, params) #ksize,sigma,mu\n",
    "\n",
    "        print('# SOLUTION: Activation.')\n",
    "        # SOLUTION: Activation.\n",
    "        pipeline.insert(self.Tasks.Activate,0)\n",
    "\n",
    "        print('# SOLUTION: Layer 5: Fully Connected. Input = 84. Output = classize.')\n",
    "        # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "        params = dict()\n",
    "        params['ksize'] = (84, classize)\n",
    "        params['sigma'] = 0.1\n",
    "        params['mu'] = 0\n",
    "        pipeline.insert(self.Tasks.FullConnect,params) #ksize,sigma,mu\n",
    "\n",
    "\n",
    "        print(pipeline)\n",
    "        self.register(pipeline)\n",
    "\n",
    "        #batch_features, batch_labels  = load_batch(0,X_train, y_train)\n",
    "\n",
    "        self.execute(images)\n",
    "\n",
    "    \n",
    "def load_batch(batch_i,train_features,train_labels):\n",
    "    batch_size = 1000 #train_features/batch_count\n",
    "    # The training cycle\n",
    "    #for batch_i in range(batch_count):\n",
    "    # Get a batch of training features and labels\n",
    "    batch_start = batch_i*batch_size\n",
    "    batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "    batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "    return batch_features, batch_labels   \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#batch_features, batch_labels  = load_batch(0,X_train, y_train)\n",
    "#x = (None, 32, 32, 1)\n",
    "#define_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "class Trainer(object):\n",
    "    def __init__(self,rate,epochs,batch_size,x,y,logits):\n",
    "        assert(not rate == None or rate>1),\"None or greater than one rate parameter is not allowed. Please, provide a rate parameter\" \n",
    "        assert(not epochs == None or epochs>=1),\"None or greater than one epochs parameter is not allowed. Please, provide a epochs parameter\" \n",
    "        assert(not batch_size == None or batch_size>1),\"None or greater than one batch_size parameter is not allowed. Please, provide a batch_size parameter\" \n",
    "        assert(not x == None),\"None or greater than one x parameter is not allowed. Please, provide a x parameter\" \n",
    "        assert(not y == None),\"None or greater than one y parameter is not allowed. Please, provide a y parameter\" \n",
    "        assert(not logits == None),\"None or greater than one logits parameter is not allowed. Please, provide a logits parameter\" \n",
    "        self.rate = rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.training_operation = None\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.y = y\n",
    "        self.x = x\n",
    "        self.logits = logits\n",
    "        self.one_hot_y = None\n",
    "        \n",
    "    def build_pipeline(self, classsize):\n",
    "        assert(not classsize == None),\"None or empty one_hot_y parameter is not allowed. Please, provide a one_hot_y parameter\" \n",
    "        self.one_hot_y = tf.one_hot(self.y, classsize)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(self.logits, self.one_hot_y)\n",
    "        loss_operation = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = self.rate)\n",
    "        self.training_operation = optimizer.minimize(loss_operation)\n",
    "        \n",
    "    \n",
    "    def evaluate(self, X_data, y_data):\n",
    "        assert(not X_data == None),\"None or empty X_data parameter is not allowed. Please, provide a X_data parameter\" \n",
    "        assert(not y_data == None),\"None or empty y_data parameter is not allowed. Please, provide a y_data parameter\" \n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.one_hot_y, 1))\n",
    "        accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        num_examples = len(X_data)\n",
    "        total_accuracy = 0\n",
    "        sess = tf.get_default_session()\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "            accuracy = sess.run(accuracy_operation, feed_dict={self.x: batch_x, self.y: batch_y})\n",
    "            total_accuracy += (accuracy * len(batch_x))\n",
    "        return total_accuracy / num_examples\n",
    "    \n",
    "    def train_model(self,X_train, y_train):\n",
    "        assert(not X_train == None),\"None or empty X_train parameter is not allowed. Please, provide a X_train parameter\" \n",
    "        assert(not y_train == None),\"None or empty y_train parameter is not allowed. Please, provide a y_train parameter\" \n",
    "        \n",
    "        da = DataAugmenter()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            num_examples = len(X_train)\n",
    "            dpp = DataPreProcessor()\n",
    "            print(\"Training...\")\n",
    "            print()\n",
    "            for i in range(self.epochs):\n",
    "                X_train, y_train = shuffle(X_train, y_train)\n",
    "                for offset in range(0, num_examples, self.batch_size):\n",
    "                    end = offset + self.batch_size\n",
    "                    #batch_x, batch_y = dpp.preprocess(X_train[offset:end],None,False), y_train[offset:end]\n",
    "                    batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "                    sess.run(self.training_operation, feed_dict={self.x: batch_x, self.y: batch_y})\n",
    "                \n",
    "                validation_size = int(num_examples * 0.3)\n",
    "                print('validation_size :',validation_size)\n",
    "                print('difference :', int(num_examples-validation_size))\n",
    "                \n",
    "                validation_start = random.randint(0,int(num_examples-validation_size))\n",
    "                validation_end = validation_start + validation_size\n",
    "                print('validation_start : ',validation_start)\n",
    "                \n",
    "                X_validation, y_validation = da.augmente(X_train[validation_start:validation_end]) , \\\n",
    "                                            y_train[validation_start:validation_end]\n",
    "                validation_accuracy = self.evaluate(X_validation, y_validation)\n",
    "                print(\"EPOCH {} ...\".format(i+1))\n",
    "                print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "                print()\n",
    "\n",
    "            self.saver.save(sess, './lenet')\n",
    "            print(\"Model saved\")\n",
    "    \n",
    "    def test_model(self,X_test, y_test):\n",
    "        assert(not X_test == None),\"None or empty X_test parameter is not allowed. Please, provide a X_test parameter\" \n",
    "        assert(not y_test == None),\"None or empty y_test parameter is not allowed. Please, provide a y_test parameter\" \n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "            test_accuracy = self.evaluate(X_test, y_test)\n",
    "            print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11459.85it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of Initialization Step #0\n",
      "The dataset deserialization process is starting...\n",
      "Download Step #1\n",
      "Repository path traffic-signs-data.zip is file! We can not save                            files because no directory path is provided.\n",
      "Uncompression Step #3\n",
      "Unzip the image file test.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|     | 1/2 [00:00<00:00,  2.58files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzip the image file train.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:02<00:00,  1.00files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of uncompression Step :  0files are uncompressed over2\n",
      "Data Loading Step #4\n",
      "Status of Data loading from pickle file Step : successful\n",
      "Data Loading Step #4\n",
      "Status of Data loading from pickle file Step : successful\n",
      "The dataset deserialization process is terminated...\n",
      "Number of training examples = 39209\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n",
      "Max size of label_per_class = [   2 2250]\n",
      "10\n",
      "# SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n",
      "# SOLUTION: Activation.\n",
      "# SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
      "# SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
      "# SOLUTION: Activation.\n",
      "# SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
      "# SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
      "# SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
      "# SOLUTION: Activation.\n",
      "# SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
      "# SOLUTION: Activation.\n",
      "# SOLUTION: Layer 5: Fully Connected. Input = 84. Output = classize.\n",
      "<__main__.Model.Container object at 0x7ff65289e710>\n",
      "Model Pipeline registration process is starting ...\n",
      "Model Pipeline registration process is terminated ...\n",
      "Model Pipeline execution process is starting ...\n",
      "_convolve__2\n",
      "tsk : _convolve\n",
      "{'sigma': 0.1, 'ksize': (5, 5, 3, 6), 'stride': [1, 1, 1, 1], 'padding': 'VALID', 'mu': 0}\n",
      "stride :  [1, 1, 1, 1]\n",
      "images : Tensor(\"Placeholder_32:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "weights : Tensor(\"Variable_160/read:0\", shape=(5, 5, 3, 6), dtype=float32)\n",
      "biais : Tensor(\"Variable_161/read:0\", shape=(6,), dtype=float32)\n",
      "tensor : Tensor(\"add_80:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "_activate__3\n",
      "tsk : _activate\n",
      "tensor : Tensor(\"Relu_64:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "_pool__4\n",
      "tsk : _pool\n",
      "tensor : Tensor(\"MaxPool_32:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "_convolve__5\n",
      "tsk : _convolve\n",
      "{'sigma': 0.1, 'ksize': (5, 5, 6, 16), 'stride': [1, 1, 1, 1], 'padding': 'VALID', 'mu': 0}\n",
      "stride :  [1, 1, 1, 1]\n",
      "images : Tensor(\"MaxPool_32:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "weights : Tensor(\"Variable_162/read:0\", shape=(5, 5, 6, 16), dtype=float32)\n",
      "biais : Tensor(\"Variable_163/read:0\", shape=(16,), dtype=float32)\n",
      "tensor : Tensor(\"add_81:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "_activate__6\n",
      "tsk : _activate\n",
      "tensor : Tensor(\"Relu_65:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "_pool__7\n",
      "tsk : _pool\n",
      "tensor : Tensor(\"MaxPool_33:0\", shape=(?, 5, 5, 16), dtype=float32)\n",
      "_flatten__8\n",
      "tsk : _flatten\n",
      "tensor : Tensor(\"Flatten_16/Reshape:0\", shape=(?, 400), dtype=float32)\n",
      "_fullconnect__9\n",
      "tsk : _fullconnect\n",
      "tensor : Tensor(\"add_82:0\", shape=(?, 120), dtype=float32)\n",
      "_activate__10\n",
      "tsk : _activate\n",
      "tensor : Tensor(\"Relu_66:0\", shape=(?, 120), dtype=float32)\n",
      "_fullconnect__11\n",
      "tsk : _fullconnect\n",
      "tensor : Tensor(\"add_83:0\", shape=(?, 84), dtype=float32)\n",
      "_activate__12\n",
      "tsk : _activate\n",
      "tensor : Tensor(\"Relu_67:0\", shape=(?, 84), dtype=float32)\n",
      "_fullconnect__13\n",
      "tsk : _fullconnect\n",
      "tensor : Tensor(\"add_84:0\", shape=(?, 43), dtype=float32)\n",
      "Model Pipeline execution process is terminated ...\n",
      "logits : "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:45: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:46: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tensor(\"add_84:0\", shape=(?, 43), dtype=float32)\n",
      "Training...\n",
      "\n",
      "validation_size : 11762\n",
      "difference : 27447\n",
      "validation_start :  7713\n",
      "Augmenter Pipeline registration process is starting ...\n",
      "Augmenter Pipeline registration process is terminated ...\n",
      "Augmenter Pipeline execution process is starting ...\n",
      "Augmenter Pipeline execution process is terminated ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.157\n",
      "\n",
      "validation_size : 11762\n",
      "difference : 27447\n",
      "validation_start :  11280\n",
      "Augmenter Pipeline registration process is starting ...\n",
      "Augmenter Pipeline registration process is terminated ...\n",
      "Augmenter Pipeline execution process is starting ...\n",
      "Augmenter Pipeline execution process is terminated ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.101\n",
      "\n",
      "validation_size : 11762\n",
      "difference : 27447\n",
      "validation_start :  12043\n",
      "Augmenter Pipeline registration process is starting ...\n",
      "Augmenter Pipeline registration process is terminated ...\n",
      "Augmenter Pipeline execution process is starting ...\n",
      "Augmenter Pipeline execution process is terminated ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.067\n",
      "\n",
      "validation_size : 11762\n",
      "difference : 27447\n",
      "validation_start :  2158\n",
      "Augmenter Pipeline registration process is starting ...\n",
      "Augmenter Pipeline registration process is terminated ...\n",
      "Augmenter Pipeline execution process is starting ...\n",
      "Augmenter Pipeline execution process is terminated ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.058\n",
      "\n",
      "validation_size : 11762\n",
      "difference : 27447\n",
      "validation_start :  25064\n",
      "Augmenter Pipeline registration process is starting ...\n",
      "Augmenter Pipeline registration process is terminated ...\n",
      "Augmenter Pipeline execution process is starting ...\n",
      "Augmenter Pipeline execution process is terminated ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.058\n",
      "\n",
      "validation_size : 11762\n",
      "difference : 27447\n",
      "validation_start :  14990\n",
      "Augmenter Pipeline registration process is starting ...\n",
      "Augmenter Pipeline registration process is terminated ...\n",
      "Augmenter Pipeline execution process is starting ...\n",
      "Augmenter Pipeline execution process is terminated ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.046\n",
      "\n",
      "validation_size : 11762\n",
      "difference : 27447\n",
      "validation_start :  20825\n",
      "Augmenter Pipeline registration process is starting ...\n",
      "Augmenter Pipeline registration process is terminated ...\n",
      "Augmenter Pipeline execution process is starting ...\n",
      "Augmenter Pipeline execution process is terminated ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.046\n",
      "\n",
      "validation_size : 11762\n",
      "difference : 27447\n",
      "validation_start :  5008\n",
      "Augmenter Pipeline registration process is starting ...\n",
      "Augmenter Pipeline registration process is terminated ...\n",
      "Augmenter Pipeline execution process is starting ...\n",
      "Augmenter Pipeline execution process is terminated ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.049\n",
      "\n",
      "validation_size : 11762\n",
      "difference : 27447\n",
      "validation_start :  14170\n",
      "Augmenter Pipeline registration process is starting ...\n",
      "Augmenter Pipeline registration process is terminated ...\n",
      "Augmenter Pipeline execution process is starting ...\n",
      "Augmenter Pipeline execution process is terminated ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/naaman/anaconda3/envs/CTSC/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.043\n",
      "\n",
      "validation_size : 11762\n",
      "difference : 27447\n",
      "validation_start :  25343\n",
      "Augmenter Pipeline registration process is starting ...\n",
      "Augmenter Pipeline registration process is terminated ...\n",
      "Augmenter Pipeline execution process is starting ...\n"
     ]
    }
   ],
   "source": [
    "sources = [source]\n",
    "repositories = [repository]\n",
    "\n",
    "ds = Dataset(sources, repositories)\n",
    "X_train, y_train,X_test, y_test = ds.deserialize()\n",
    "print(\"Number of training examples =\", ds.train_size)\n",
    "print(\"Number of testing examples =\", ds.test_size)\n",
    "print(\"Image data shape =\", ds.image_shape)\n",
    "print(\"Number of classes =\", ds.class_size)\n",
    "#print(\"Number of label_per_class =\", ds.label_per_class)\n",
    "print(\"Max size of label_per_class =\", ds.max_size_feature)\n",
    "ds.visualize(X_train, y_train)\n",
    "#args = (None,True)\n",
    "# X_train, y_train = ds.balance(X_train,y_train,partial(apply_augmentation2),args)\n",
    "#X_train, y_train =  apply_preprocessing(X_train, y_train,True) \n",
    "model = Model()\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "model.build(x,ds.class_size)\n",
    "print('logits : ', model.logits)\n",
    "trainer = Trainer(rate,EPOCHS,BATCH_SIZE,x,y,model.logits)\n",
    "trainer.build_pipeline(ds.class_size)\n",
    "trainer.train_model(X_train, y_train)\n",
    "trainer.test_model(X_test, y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
